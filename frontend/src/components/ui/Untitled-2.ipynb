{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any, List, TypedDict\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Assume these imports and functions are available\n",
    "from your_modules import LLMGateway, run_vespa_search, get_vespa_payload, parse_json_safely\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    entities: List[str]\n",
    "    graph_data: Dict[str, Any]\n",
    "    expanded_query: str\n",
    "    results: Dict[str, Any]\n",
    "    next_steps: List[str]\n",
    "\n",
    "llm = LLMGateway(model_name='gpt-4o')\n",
    "\n",
    "def entity_recognition_and_router(state: GraphState) -> Dict[str, Any]:\n",
    "    # Implement entity recognition logic\n",
    "    entities = [\"entity1\", \"entity2\"]  # Placeholder for entity recognition\n",
    "    \n",
    "    # Determine if we need to use graph DB\n",
    "    if entities and need_graph_enrichment(entities):  # Implement need_graph_enrichment function\n",
    "        return {\n",
    "            \"entities\": entities,\n",
    "            \"next_steps\": [\"graph_rag_enrichment\"]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"entities\": entities,\n",
    "            \"next_steps\": [\"query_expansion\"]\n",
    "        }\n",
    "\n",
    "def graph_rag_enrichment(state: GraphState) -> Dict[str, Any]:\n",
    "    # Query graph database and extract relationships\n",
    "    graph_data = {\"related_entities\": [\"related1\", \"related2\"]}  # Placeholder\n",
    "    return {\"graph_data\": graph_data}\n",
    "\n",
    "def query_expansion(state: GraphState) -> Dict[str, Any]:\n",
    "    # Expand query based on entities and graph data\n",
    "    expanded_terms = state['entities'] + state.get('graph_data', {}).get('related_entities', [])\n",
    "    expanded_query = f\"{state['query']} AND ({' OR '.join(expanded_terms)})\"\n",
    "    return {\"expanded_query\": expanded_query}\n",
    "\n",
    "def query_optimization(state: GraphState) -> Dict[str, Any]:\n",
    "    # Optimize the expanded query\n",
    "    optimized_query = state[\"expanded_query\"]  # Placeholder for optimization logic\n",
    "    return {\"optimized_query\": optimized_query}\n",
    "\n",
    "# ECT Chain (Earning Call Transcripts)\n",
    "def create_ect_chain(llm, ect_schema_prompt: ChatPromptTemplate, ect_prompt: ChatPromptTemplate):\n",
    "    # Implementation remains the same as before\n",
    "    ...\n",
    "\n",
    "ect_schema_prompt = ChatPromptTemplate.from_template(\"Your ECT_SCHEMA template here\")\n",
    "ect_prompt = ChatPromptTemplate.from_template(\"Your ECT_PROMPT template here\")\n",
    "ect_chain = create_ect_chain(llm, ect_schema_prompt, ect_prompt)\n",
    "\n",
    "# Analyst Commentary Chain\n",
    "def create_analyst_commentary_chain(llm, analyst_schema_prompt: ChatPromptTemplate, analyst_prompt: ChatPromptTemplate):\n",
    "    # Similar structure to ect_chain, but for analyst commentary database\n",
    "    ...\n",
    "\n",
    "analyst_schema_prompt = ChatPromptTemplate.from_template(\"Your ANALYST_SCHEMA template here\")\n",
    "analyst_prompt = ChatPromptTemplate.from_template(\"Your ANALYST_PROMPT template here\")\n",
    "analyst_chain = create_analyst_commentary_chain(llm, analyst_schema_prompt, analyst_prompt)\n",
    "\n",
    "def parallel_vector_search(state: GraphState) -> Dict[str, Any]:\n",
    "    ect_results = ect_chain.invoke({\"query\": state[\"optimized_query\"]})\n",
    "    analyst_results = analyst_chain.invoke({\"query\": state[\"optimized_query\"]})\n",
    "    # Add more vector database searches as needed\n",
    "    \n",
    "    return {\n",
    "        \"results\": {\n",
    "            \"ect\": ect_results,\n",
    "            \"analyst\": analyst_results\n",
    "        }\n",
    "    }\n",
    "\n",
    "def result_aggregation(state: GraphState) -> Dict[str, Any]:\n",
    "    # Aggregate results from different vector searches\n",
    "    aggregated_results = {\n",
    "        \"ect\": summarize_results(state[\"results\"][\"ect\"]),\n",
    "        \"analyst\": summarize_results(state[\"results\"][\"analyst\"])\n",
    "    }\n",
    "    return {\"aggregated_results\": aggregated_results}\n",
    "\n",
    "def context_integration(state: GraphState) -> Dict[str, Any]:\n",
    "    # Integrate context from graph data and aggregated results\n",
    "    context_integrated_results = {\n",
    "        \"results\": state[\"aggregated_results\"],\n",
    "        \"graph_context\": state.get(\"graph_data\", {})\n",
    "    }\n",
    "    return {\"context_integrated_results\": context_integrated_results}\n",
    "\n",
    "def final_response_generation(state: GraphState) -> Dict[str, Any]:\n",
    "    # Generate final response using LLM\n",
    "    response_prompt = PromptTemplate.from_template(\n",
    "        \"Based on the following information, provide a comprehensive answer to the query: {query}\\n\\n\"\n",
    "        \"Earning Call Transcript Summary: {ect_summary}\\n\"\n",
    "        \"Analyst Commentary Summary: {analyst_summary}\\n\"\n",
    "        \"Additional Context: {graph_context}\\n\\n\"\n",
    "        \"Comprehensive Answer:\"\n",
    "    )\n",
    "    \n",
    "    final_response = llm(response_prompt.format(\n",
    "        query=state[\"query\"],\n",
    "        ect_summary=state[\"context_integrated_results\"][\"results\"][\"ect\"],\n",
    "        analyst_summary=state[\"context_integrated_results\"][\"results\"][\"analyst\"],\n",
    "        graph_context=state[\"context_integrated_results\"][\"graph_context\"]\n",
    "    ))\n",
    "    \n",
    "    return {\"final_response\": final_response}\n",
    "\n",
    "def feedback_collection(state: GraphState) -> Dict[str, Any]:\n",
    "    # Collect feedback (this would typically be an external process)\n",
    "    return {\"feedback\": \"Placeholder feedback\"}\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"initial_query\", RunnableLambda(lambda x: x))\n",
    "workflow.add_node(\"entity_recognition_and_router\", entity_recognition_and_router)\n",
    "workflow.add_node(\"graph_rag_enrichment\", graph_rag_enrichment)\n",
    "workflow.add_node(\"query_expansion\", query_expansion)\n",
    "workflow.add_node(\"query_optimization\", query_optimization)\n",
    "workflow.add_node(\"parallel_vector_search\", parallel_vector_search)\n",
    "workflow.add_node(\"result_aggregation\", result_aggregation)\n",
    "workflow.add_node(\"context_integration\", context_integration)\n",
    "workflow.add_node(\"final_response_generation\", final_response_generation)\n",
    "workflow.add_node(\"feedback_collection\", feedback_collection)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"initial_query\", \"entity_recognition_and_router\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"entity_recognition_and_router\",\n",
    "    lambda x: x[\"next_steps\"],\n",
    "    {\n",
    "        \"graph_rag_enrichment\": \"graph_rag_enrichment\",\n",
    "        \"query_expansion\": \"query_expansion\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"graph_rag_enrichment\", \"query_expansion\")\n",
    "workflow.add_edge(\"query_expansion\", \"query_optimization\")\n",
    "workflow.add_edge(\"query_optimization\", \"parallel_vector_search\")\n",
    "workflow.add_edge(\"parallel_vector_search\", \"result_aggregation\")\n",
    "workflow.add_edge(\"result_aggregation\", \"context_integration\")\n",
    "workflow.add_edge(\"context_integration\", \"final_response_generation\")\n",
    "workflow.add_edge(\"final_response_generation\", \"feedback_collection\")\n",
    "workflow.add_edge(\"feedback_collection\", END)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"initial_query\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the graph\n",
    "query = \"What are the latest advancements in quantum computing?\"\n",
    "initial_state = {\"query\": query, \"entities\": [], \"graph_data\": {}, \"expanded_query\": \"\", \"results\": {}, \"next_steps\": []}\n",
    "result = graph.invoke(initial_state)\n",
    "print(f\"Final result: {result['final_response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
