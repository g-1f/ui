{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from agent import Agent\n",
    "\n",
    "# Initialize components\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "# Define the state structure\n",
    "class GraphState(Dict):\n",
    "    query: str\n",
    "    results: Dict[str, str]\n",
    "\n",
    "# Node functions\n",
    "def node_a(state: GraphState) -> GraphState:\n",
    "    print(\"Executing node_a\")\n",
    "    return state\n",
    "\n",
    "async def search_and_generate(node: str, state: GraphState, prompt: PromptTemplate) -> Dict[str, str]:\n",
    "    print(f\"Executing {node}\")\n",
    "    # Perform search\n",
    "    search_results = await run_vespa_search(state[\"query\"], top_k=5, tags=node)\n",
    "    \n",
    "    # Generate response with LLM\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\": state[\"query\"], \"search_results\": search_results})\n",
    "    return {node: response}\n",
    "\n",
    "def create_node_function(node: str, prompt: PromptTemplate):\n",
    "    def node_function(state: GraphState) -> GraphState:\n",
    "        result = asyncio.run(search_and_generate(node, state, prompt))\n",
    "        state[\"results\"].update(result)\n",
    "        return state\n",
    "    return node_function\n",
    "\n",
    "# Define prompts for each node\n",
    "prompt_b = PromptTemplate.from_template(\n",
    "    \"Analyze the following search results for the query: {query}\\n\\nSearch Results: {search_results}\\n\\nProvide a concise summary focusing on the main points.\"\n",
    ")\n",
    "\n",
    "prompt_c = PromptTemplate.from_template(\n",
    "    \"Given the query: {query}\\n\\nAnd the search results: {search_results}\\n\\nIdentify any conflicting information or controversies in the results.\"\n",
    ")\n",
    "\n",
    "prompt_d = PromptTemplate.from_template(\n",
    "    \"For the query: {query}\\n\\nBased on these search results: {search_results}\\n\\nProvide potential future developments or implications.\"\n",
    ")\n",
    "\n",
    "# Create node functions\n",
    "node_b = create_node_function(\"b\", prompt_b)\n",
    "node_c = create_node_function(\"c\", prompt_c)\n",
    "node_d = create_node_function(\"d\", prompt_d)\n",
    "\n",
    "def aggregate_results(state: GraphState) -> GraphState:\n",
    "    print(\"Aggregating results\")\n",
    "    combined_response = \"\\n\".join([f\"Node {k}: {v}\" for k, v in state[\"results\"].items()])\n",
    "    \n",
    "    aggregate_prompt = PromptTemplate.from_template(\n",
    "        \"Synthesize a comprehensive answer based on these results:\\n{combined_response}\\n\\nProvide a well-structured and coherent response that addresses the original query: {query}\"\n",
    "    )\n",
    "    \n",
    "    chain = aggregate_prompt | llm\n",
    "    state[\"final_result\"] = chain.invoke({\"combined_response\": combined_response, \"query\": state[\"query\"]})\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"a\", node_a)\n",
    "workflow.add_node(\"b\", node_b)\n",
    "workflow.add_node(\"c\", node_c)\n",
    "workflow.add_node(\"d\", node_d)\n",
    "workflow.add_node(\"aggregate\", aggregate_results)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"a\", \"b\")\n",
    "workflow.add_edge(\"a\", \"c\")\n",
    "workflow.add_edge(\"a\", \"d\")\n",
    "workflow.set_entry_point(\"a\")\n",
    "workflow.add_edge(\"b\", \"aggregate\")\n",
    "workflow.add_edge(\"c\", \"aggregate\")\n",
    "workflow.add_edge(\"d\", \"aggregate\")\n",
    "workflow.add_edge(\"aggregate\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the graph\n",
    "query = \"What are the latest advancements in quantum computing?\"\n",
    "result = graph.invoke({\"query\": query, \"results\": {}})\n",
    "print(f\"Final result: {result['final_result']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
