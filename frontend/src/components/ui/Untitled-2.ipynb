{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema.runnable'; 'langchain.schema' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Any, List, TypedDict\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough, RunnableLambda, RunnableParallel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate, ChatPromptTemplate\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.schema.runnable'; 'langchain.schema' is not a package"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import Graph, END\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Dict, List, Any\n",
    "import asyncio\n",
    "from neo4j import AsyncGraphDatabase  # Assuming use of neo4j for graph database\n",
    "\n",
    "# Import subgraph implementations\n",
    "from subgraphs import run_ect_subgraph, run_ac_subgraph\n",
    "\n",
    "# Assume llm is imported and configured\n",
    "from your_llm_config import llm\n",
    "\n",
    "class AgentState(Dict):\n",
    "    query: str\n",
    "    processed_query: str\n",
    "    use_graph_db: bool\n",
    "    graph_db_result: Dict\n",
    "    expanded_queries: List[str]\n",
    "    subgraphs_to_execute: Dict[str, List[str]]  # Maps queries to subgraphs\n",
    "    subgraph_results: Dict[str, Any]\n",
    "    final_response: str\n",
    "\n",
    "# ... (keep preprocess_query function as before)\n",
    "\n",
    "async def graph_db_router(state: AgentState) -> AgentState:\n",
    "    router_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Analyze the following query and determine if it can utilize graph relationships:\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Respond with either 'Yes' or 'No' followed by a brief explanation.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    result = await router_prompt.aformat(query=state[\"processed_query\"]) | llm | str\n",
    "    state[\"use_graph_db\"] = result.lower().startswith(\"yes\")\n",
    "    return state\n",
    "\n",
    "async def query_graph_db(state: AgentState) -> AgentState:\n",
    "    if not state[\"use_graph_db\"]:\n",
    "        return state\n",
    "\n",
    "    # Replace with your actual Neo4j connection details\n",
    "    uri = \"neo4j://localhost:7687\"\n",
    "    user = \"neo4j\"\n",
    "    password = \"password\"\n",
    "\n",
    "    async with AsyncGraphDatabase.driver(uri, auth=(user, password)) as driver:\n",
    "        async with driver.session() as session:\n",
    "            # This is a placeholder Cypher query. Replace with actual query generation logic.\n",
    "            cypher_query = f\"MATCH (n) WHERE n.name CONTAINS '{state['processed_query']}' RETURN n LIMIT 5\"\n",
    "            result = await session.run(cypher_query)\n",
    "            state[\"graph_db_result\"] = [record.data() for record in await result.data()]\n",
    "\n",
    "    return state\n",
    "\n",
    "async def expand_queries(state: AgentState) -> AgentState:\n",
    "    expansion_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Based on the original query and the graph database results, generate a list of expanded queries \n",
    "        that cover different aspects of the information needed. Separate queries with commas.\n",
    "\n",
    "        Original Query: {query}\n",
    "        Graph Database Results: {graph_results}\n",
    "\n",
    "        Expanded Queries:\"\"\"\n",
    "    )\n",
    "\n",
    "    expanded_queries = await expansion_prompt.aformat(\n",
    "        query=state[\"processed_query\"],\n",
    "        graph_results=state[\"graph_db_result\"]\n",
    "    ) | llm | CommaSeparatedListOutputParser()\n",
    "\n",
    "    state[\"expanded_queries\"] = expanded_queries\n",
    "    return state\n",
    "\n",
    "async def subgraph_router(state: AgentState) -> AgentState:\n",
    "    router_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"For each of the following queries, determine which subgraphs should be executed.\n",
    "        Available subgraphs: ect_subgraph (Earning Call Transcripts), ac_subgraph (Analyst Commentary).\n",
    "        \n",
    "        Queries:\n",
    "        {queries}\n",
    "        \n",
    "        Respond with a JSON object where keys are the queries and values are lists of subgraphs to execute.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    routing_result = await router_prompt.aformat(queries=\"\\n\".join(state[\"expanded_queries\"])) | llm | str\n",
    "    # Assume the LLM returns a valid JSON string. In practice, you might need more robust parsing.\n",
    "    state[\"subgraphs_to_execute\"] = eval(routing_result)\n",
    "    return state\n",
    "\n",
    "async def execute_subgraphs(state: AgentState) -> AgentState:\n",
    "    subgraph_map = {\n",
    "        \"ect_subgraph\": run_ect_subgraph,\n",
    "        \"ac_subgraph\": run_ac_subgraph\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for query, subgraphs in state[\"subgraphs_to_execute\"].items():\n",
    "        query_results = {}\n",
    "        for subgraph in subgraphs:\n",
    "            if subgraph in subgraph_map:\n",
    "                query_results[subgraph] = await subgraph_map[subgraph](query, state[\"graph_db_result\"])\n",
    "        results[query] = query_results\n",
    "    \n",
    "    state[\"subgraph_results\"] = results\n",
    "    return state\n",
    "\n",
    "async def aggregate_results(state: AgentState) -> AgentState:\n",
    "    aggregation_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Synthesize a comprehensive response based on the following information:\n",
    "        \n",
    "        Original Query: {original_query}\n",
    "        Graph Database Results: {graph_db_result}\n",
    "        Expanded Queries and Subgraph Results: {subgraph_results}\n",
    "        \n",
    "        Provide a coherent and informative response that addresses the original query.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    state[\"final_response\"] = await aggregation_prompt.aformat(\n",
    "        original_query=state[\"query\"],\n",
    "        graph_db_result=state[\"graph_db_result\"],\n",
    "        subgraph_results=state[\"subgraph_results\"]\n",
    "    ) | llm | str\n",
    "    \n",
    "    return state\n",
    "\n",
    "def create_finance_agent_graph():\n",
    "    workflow = Graph()\n",
    "\n",
    "    workflow.add_node(\"preprocess\", RunnableLambda(preprocess_query))\n",
    "    workflow.add_node(\"graph_db_router\", RunnableLambda(graph_db_router))\n",
    "    workflow.add_node(\"query_graph_db\", RunnableLambda(query_graph_db))\n",
    "    workflow.add_node(\"expand_queries\", RunnableLambda(expand_queries))\n",
    "    workflow.add_node(\"subgraph_router\", RunnableLambda(subgraph_router))\n",
    "    workflow.add_node(\"execute_subgraphs\", RunnableLambda(execute_subgraphs))\n",
    "    workflow.add_node(\"aggregate_results\", RunnableLambda(aggregate_results))\n",
    "\n",
    "    workflow.add_edge(\"preprocess\", \"graph_db_router\")\n",
    "    workflow.add_edge(\"graph_db_router\", \"query_graph_db\")\n",
    "    workflow.add_edge(\"query_graph_db\", \"expand_queries\")\n",
    "    workflow.add_edge(\"expand_queries\", \"subgraph_router\")\n",
    "    workflow.add_edge(\"subgraph_router\", \"execute_subgraphs\")\n",
    "    workflow.add_edge(\"execute_subgraphs\", \"aggregate_results\")\n",
    "    workflow.add_edge(\"aggregate_results\", END)\n",
    "\n",
    "    workflow.set_entry_point(\"preprocess\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "async def run_finance_agent(query: str):\n",
    "    graph = create_finance_agent_graph()\n",
    "    initial_state = AgentState(\n",
    "        query=query,\n",
    "        processed_query=\"\",\n",
    "        use_graph_db=False,\n",
    "        graph_db_result={},\n",
    "        expanded_queries=[],\n",
    "        subgraphs_to_execute={},\n",
    "        subgraph_results={},\n",
    "        final_response=\"\"\n",
    "    )\n",
    "    result = await graph.ainvoke(initial_state)\n",
    "    return result[\"final_response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
