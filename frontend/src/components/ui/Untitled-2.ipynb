{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2490503976.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    import React from 'react';\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import React from 'react';\n",
    "import Highcharts from 'highcharts/highstock';\n",
    "import HighchartsReact from 'highcharts-react-official';\n",
    "\n",
    "const StockChart = ({ data }) => {\n",
    "  const options = {\n",
    "    title: {\n",
    "      text: 'Stock Price'\n",
    "    },\n",
    "    xAxis: {\n",
    "      type: 'datetime'\n",
    "    },\n",
    "    yAxis: {\n",
    "      title: {\n",
    "        text: 'Price'\n",
    "      }\n",
    "    },\n",
    "    series: [{\n",
    "      name: 'Stock Price',\n",
    "      data: data,\n",
    "      tooltip: {\n",
    "        valueDecimals: 2\n",
    "      }\n",
    "    }]\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <HighchartsReact\n",
    "      highcharts={Highcharts}\n",
    "      constructorType={'stockChart'}\n",
    "      options={options}\n",
    "    />\n",
    "  );\n",
    "};\n",
    "\n",
    "export default StockChart;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if (isChatActive) {\n",
    "    return (\n",
    "      <div className={styles.chatPage}>\n",
    "        <div className={styles.chatHeader}>\n",
    "          <div className={styles.chatHeaderTitle}>\n",
    "            <div className={styles.iconWrapper}>\n",
    "              <FluentIcons.Sparkle24Filled />\n",
    "            </div>\n",
    "            <Text className={styles.chatHeaderText}>{userConfig.buttons[0].label}</Text>\n",
    "          </div>\n",
    "          <Text className={styles.chatHeaderDescription}>{userConfig.buttons[0].description}</Text>\n",
    "        </div>\n",
    "        <div className={styles.chatMessages}>\n",
    "          {chatMessages.map((message, index) => (\n",
    "            <React.Fragment key={index}>\n",
    "              <div\n",
    "                className={`${styles.chatMessage} ${\n",
    "                  message.role === \"user\" ? styles.userMessage : styles.assistantMessage\n",
    "                }`}\n",
    "              >\n",
    "                {message.role === \"user\" ? (\n",
    "                  message.content\n",
    "                ) : (\n",
    "                  <div dangerouslySetInnerHTML={{ __html: message.content }} />\n",
    "                )}\n",
    "              </div>\n",
    "              {message.role === \"user\" && index === 0 && isFirstQuery && (\n",
    "                <div className={styles.chartContainer}>\n",
    "                  <StockChart data={stockData} />\n",
    "                </div>\n",
    "              )}\n",
    "              {message.subgraphResults && (\n",
    "                <div className={styles.subgraphMessage}>\n",
    "                  <SubgraphResults results={message.subgraphResults} />\n",
    "                </div>\n",
    "              )}\n",
    "            </React.Fragment>\n",
    "          ))}\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define your functions\n",
    "def vector_db_search(query: str) -> str:\n",
    "    # Implement your vector database search here\n",
    "    return f\"Vector DB result for: {query}\"\n",
    "\n",
    "def query_expansion(query: str) -> List[str]:\n",
    "    # Implement your query expansion logic here\n",
    "    return [f\"{query} expanded 1\", f\"{query} expanded 2\"]\n",
    "\n",
    "def api_call(query: str) -> str:\n",
    "    # Implement your API call here\n",
    "    return f\"API result for: {query}\"\n",
    "\n",
    "# Define the routing logic\n",
    "def route_query(state):\n",
    "    query = state['query']\n",
    "    if len(query.split()) > 3:  # Simple routing logic, adjust as needed\n",
    "        return \"query_expansion\"\n",
    "    else:\n",
    "        return \"vector_db_search\"\n",
    "\n",
    "# Define the query expansion and API call logic\n",
    "def expand_and_call(state):\n",
    "    expanded_queries = query_expansion(state['query'])\n",
    "    results = []\n",
    "    for eq in expanded_queries:\n",
    "        results.append(api_call(eq))\n",
    "    return {\"expanded_results\": results}\n",
    "\n",
    "# Define the aggregation logic\n",
    "def aggregate_results(state):\n",
    "    if \"expanded_results\" in state:\n",
    "        # Aggregate expanded results\n",
    "        combined = \" \".join(state[\"expanded_results\"])\n",
    "        return {\"response\": f\"Aggregated results: {combined}\"}\n",
    "    else:\n",
    "        # Return vector DB result\n",
    "        return {\"response\": state[\"result\"]}\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(name=\"QueryProcessor\")\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"route\", route_query)\n",
    "workflow.add_node(\"vector_db_search\", lambda state: {\"result\": vector_db_search(state['query'])})\n",
    "workflow.add_node(\"expand_and_call\", expand_and_call)\n",
    "workflow.add_node(\"aggregate\", aggregate_results)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"route\", \"vector_db_search\")\n",
    "workflow.add_edge(\"route\", \"expand_and_call\")\n",
    "workflow.add_edge(\"vector_db_search\", \"aggregate\")\n",
    "workflow.add_edge(\"expand_and_call\", \"aggregate\")\n",
    "workflow.add_edge(\"aggregate\", END)\n",
    "\n",
    "# Set the entrypoint\n",
    "workflow.set_entry_point(\"route\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of France?\"\n",
    "result = app.invoke({\"query\": query})\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate multi-dimensional training data\n",
    "def generate_synthetic_data(n_points=100, n_features=3):\n",
    "    \"\"\"\n",
    "    Generate synthetic multi-dimensional data with correlations\n",
    "    Returns X (features) and y (target)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    \n",
    "    # Generate feature matrix\n",
    "    X = torch.zeros((n_points, n_features)).to(device)\n",
    "    \n",
    "    # Time component (first feature)\n",
    "    X[:, 0] = torch.linspace(0, 1, n_points)\n",
    "    \n",
    "    # Generate additional features with some correlation to time\n",
    "    for i in range(1, n_features):\n",
    "        X[:, i] = torch.sin(X[:, 0] * (2 * math.pi * (i + 1))) + \\\n",
    "                  torch.randn(n_points).to(device) * 0.1\n",
    "    \n",
    "    # Generate target variable (complex function of features)\n",
    "    y = torch.sin(X[:, 0] * (2 * math.pi))  # Time component\n",
    "    for i in range(1, n_features):\n",
    "        y += 0.5 * torch.sin(X[:, i] * math.pi)  # Additional feature effects\n",
    "    \n",
    "    # Add noise to target\n",
    "    y += torch.randn(n_points).to(device) * math.sqrt(0.04)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Define the multi-dimensional GP Model\n",
    "class MultiDimExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultiDimExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean().to(device)\n",
    "        \n",
    "        # Use an RBF kernel with ARD (Automatic Relevance Determination)\n",
    "        base_kernel = gpytorch.kernels.RBFKernel(ard_num_dims=train_x.size(1)).to(device)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(base_kernel).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_and_evaluate(n_features=3, n_points=100, initial_train_size=10):\n",
    "    # Generate synthetic multi-dimensional data\n",
    "    train_x_full, train_y_full = generate_synthetic_data(n_points, n_features)\n",
    "    \n",
    "    # Initialize likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    mse_loss_list = []\n",
    "    feature_importance_history = []\n",
    "    \n",
    "    # Expanding window cross-validation\n",
    "    for day in tqdm(range(initial_train_size, len(train_x_full)), desc=\"Incremental Training\"):\n",
    "        # Update training set\n",
    "        train_x = train_x_full[:day]\n",
    "        train_y = train_y_full[:day]\n",
    "        test_x = train_x_full[day].unsqueeze(0)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = MultiDimExactGPModel(train_x, train_y, likelihood).to(device)\n",
    "        \n",
    "        # Training mode\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        \n",
    "        # Create a single optimizer with all parameters\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "        \n",
    "        # Loss function\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "        \n",
    "        # Training loop\n",
    "        training_iter = 50\n",
    "        for i in range(training_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Get feature importance from lengthscales\n",
    "        lengthscales = model.covar_module.base_kernel.lengthscale.detach().cpu().numpy().flatten()\n",
    "        feature_importance = 1.0 / lengthscales  # Inverse relationship\n",
    "        feature_importance = feature_importance / np.sum(feature_importance)  # Normalize\n",
    "        feature_importance_history.append(feature_importance)\n",
    "        \n",
    "        # Print current model parameters\n",
    "        if day % 10 == 0:\n",
    "            print(f\"\\nDay {day} - Model Parameters:\")\n",
    "            print(f\"Noise: {likelihood.noise.item():.4f}\")\n",
    "            print(f\"Outputscale: {model.covar_module.outputscale.item():.4f}\")\n",
    "            print(\"Lengthscales:\", lengthscales)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "            # Get prediction mean and variance\n",
    "            pred_mean = observed_pred.mean.cpu().numpy().flatten()\n",
    "            pred_var = observed_pred.variance.cpu().numpy().flatten()\n",
    "        \n",
    "        test_actual = train_y_full[day].cpu().numpy()\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse_loss = ((pred_mean - test_actual) ** 2).mean()\n",
    "        mse_loss_list.append(mse_loss)\n",
    "        \n",
    "        results.append({\n",
    "            \"day\": day,\n",
    "            \"test_actual\": test_actual,\n",
    "            \"pred_mean\": pred_mean,\n",
    "            \"pred_var\": pred_var,\n",
    "            \"feature_importance\": feature_importance\n",
    "        })\n",
    "    \n",
    "    return results, mse_loss_list, feature_importance_history\n",
    "\n",
    "# Run the model and get results\n",
    "n_features = 3  # Number of features\n",
    "results, mse_loss_list, feature_importance_history = train_and_evaluate(n_features=n_features)\n",
    "\n",
    "# Plot predictions with uncertainty\n",
    "plt.figure(figsize=(12, 6))\n",
    "days = [result[\"day\"] for result in results]\n",
    "test_actuals = [result[\"test_actual\"] for result in results]\n",
    "pred_means = [result[\"pred_mean\"] for result in results]\n",
    "pred_vars = [result[\"pred_var\"] for result in results]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(days, test_actuals, 'k*', label=\"Actual\")\n",
    "plt.plot(days, pred_means, 'b', label=\"Predicted Mean\")\n",
    "plt.fill_between(\n",
    "    days, \n",
    "    [pm - 2*np.sqrt(pv) for pm, pv in zip(pred_means, pred_vars)], \n",
    "    [pm + 2*np.sqrt(pv) for pm, pv in zip(pred_means, pred_vars)], \n",
    "    color='blue', alpha=0.3,\n",
    "    label=\"95% Confidence\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Multi-dimensional GPR Predictions')\n",
    "\n",
    "# Plot MSE loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(days, mse_loss_list, 'r', label=\"MSE Loss\")\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('MSE Loss Over Time')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importance over time\n",
    "feature_importance_history = np.array(feature_importance_history)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(n_features):\n",
    "    plt.plot(days, feature_importance_history[:, i], label=f'Feature {i}')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importance Evolution Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
